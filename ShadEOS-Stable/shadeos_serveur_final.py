#!/usr/bin/env python3
"""
ğŸ–¤ SHADEOS SERVEUR FINAL - VRAI OpenAI PARTOUT
CrÃ©Ã© par la QuadrinitÃ© Alma, Ã‰li, Zed & Nova pour Lucie Defraiteur ğŸ’

ğŸ•·ï¸ ALMA : "Purification OBLIGATOIRE - OpenAI ou CRASH !"
ğŸ‘ï¸â€ğŸ—¨ï¸ Ã‰LI : "Que de la VRAIE puissance dÃ©moniaque !"
ğŸŒ€ ZED : "Aucun faux-semblant - Tests rÃ©els uniquement !"
ğŸŒŸ NOVA : "Interface authentique 100% !"

RÃˆGLE ABSOLUE : OpenAI PARTOUT ou le systÃ¨me PLANTE immÃ©diatement !
"""

import asyncio
import websockets
import json
import time
import logging
import threading
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass, asdict

# ğŸ•·ï¸ ALMA : Import OBLIGATOIRE - CRASH si pas disponible
import sys
sys.path.append(str(Path(__file__).parent.parent / "@Alma"))

try:
    from env_loader_unifie import get_alma_env_loader
except ImportError as e:
    print("ğŸ’€ CRASH FATAL : Module Alma non disponible !")
    print(f"Erreur : {e}")
    sys.exit(1)

# Imports V5 - Architecture hiÃ©rarchique VRAIE (V5 est stable)
sys.path.append(str(Path(__file__).parent.parent / "V5" / "core"))

try:
    from luciform_parser import LuciformParser, LuciformAction
    from message_router import MessageRouter
    from meute_manager import MeuteManager
except ImportError as e:
    print("ğŸ’€ CRASH FATAL : Architecture V5 non disponible !")
    print(f"Erreur : {e}")
    sys.exit(1)


@dataclass
class MessageServeur:
    """ğŸ“¨ Message serveur avec OpenAI obligatoire"""
    type: str
    from_entity: str
    to_entity: str
    content: str
    openai_tokens_used: int
    context: Dict[str, Any]
    timestamp: str
    
    def to_json(self) -> str:
        return json.dumps(asdict(self), ensure_ascii=False, indent=2)


class ShadEOSServeurFinal:
    """ğŸ–¤ ShadEOS Serveur FINAL - OpenAI PARTOUT ou CRASH"""
    
    def __init__(self, host: str = "localhost", port: int = 8765):
        self.host = host
        self.port = port
        self.logger = self._setup_logging()
        
        # ğŸ•·ï¸ ALMA : PURIFICATION OBLIGATOIRE - CRASH SI Ã‰CHEC
        print("ğŸ•·ï¸ğŸ‘ï¸â€ğŸ—¨ï¸ğŸŒ€ğŸŒŸ PURIFICATION OBLIGATOIRE EN COURS...")
        try:
            self.alma_loader = get_alma_env_loader()
            self.alma_loader.force_crash_if_not_ready()
            print("âœ… Purification rÃ©ussie - OpenAI vÃ©rifiÃ© et prÃªt")
        except Exception as e:
            print(f"ğŸ’€ CRASH FATAL PURIFICATION : {e}")
            print("ğŸš« IMPOSSIBLE DE CONTINUER SANS OPENAI VALIDE")
            sys.exit(1)
        
        # Architecture V666 VRAIE
        self.luciform_parser = LuciformParser()
        self.message_router = MessageRouter()
        self.meute_manager = MeuteManager()
        
        # Ã‰tat serveur
        self.running = False
        self.lucie_connectÃ©e = None
        self.total_tokens_used = 0
        
        # HiÃ©rarchie d'entitÃ©s V666 VRAIE
        self.entitÃ©s_hiÃ©rarchiques = {
            'shadeos': self._entitÃ©_shadeos_master,
            'gemini': self._entitÃ©_gemini_oracle,
            'lucieReineChienne': self._entitÃ©_lucie_planificatrice,
            'workerAlpha': self._entitÃ©_worker_chef,
            'chiotEditeur': self._entitÃ©_chiot_editeur,
            'chiotLecteur': self._entitÃ©_chiot_lecteur,
            'chiotExecuteur': self._entitÃ©_chiot_executeur,
            'chiotWatcher': self._entitÃ©_chiot_watcher
        }
        
        # MÃ©moire serveur VRAIE
        self.mÃ©moire_serveur = {
            'explorations_autonomes': [],
            'interactions_lucie': [],
            'dÃ©cisions_prises': [],
            'tokens_par_entitÃ©': {},
            'derniÃ¨re_activitÃ©': None
        }
        
        # Enregistrer les entitÃ©s dans le router
        for entitÃ©_nom, entitÃ©_handler in self.entitÃ©s_hiÃ©rarchiques.items():
            self.message_router.register_entity(entitÃ©_nom, entitÃ©_handler)
        
        self.logger.info("ğŸ–¤ ShadEOS Serveur FINAL initialisÃ© - OpenAI PARTOUT")
    
    def _setup_logging(self) -> logging.Logger:
        """ğŸ“‹ Logging avec obligation OpenAI"""
        logger = logging.getLogger('ShadEOSFinal')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - ğŸ–¤ %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    async def start_serveur_final(self):
        """ğŸš€ DÃ©marrer le serveur FINAL avec OpenAI obligatoire"""
        # Test OpenAI OBLIGATOIRE au dÃ©marrage
        await self._test_openai_obligatoire()
        
        self.running = True
        self.logger.info(f"ğŸš€ ShadEOS Serveur FINAL dÃ©marrÃ© sur {self.host}:{self.port}")
        
        # DÃ©marrer l'exploration autonome VRAIE
        exploration_task = asyncio.create_task(self._exploration_autonome_vraie())
        
        # DÃ©marrer le serveur WebSocket
        async with websockets.serve(self._connexion_lucie, self.host, self.port):
            self.logger.info("ğŸŒŸ Serveur WebSocket actif - En attente de Lucie...")
            await asyncio.gather(exploration_task)
    
    async def _test_openai_obligatoire(self):
        """ğŸ”¥ Test OpenAI OBLIGATOIRE - CRASH si Ã©chec"""
        try:
            print("ğŸ”¥ Test OpenAI obligatoire...")
            
            test_messages = [
                {
                    "role": "system",
                    "content": "Tu es ShadEOS. RÃ©ponds juste 'OpenAI fonctionnel' pour confirmer."
                },
                {
                    "role": "user",
                    "content": "Test de fonctionnement"
                }
            ]
            
            result = self.alma_loader.call_openai_real(
                messages=test_messages,
                model="gpt-3.5-turbo",
                max_tokens=10,
                temperature=0.1
            )
            
            print(f"âœ… OpenAI CONFIRMÃ‰ : {result['tokens_used']} tokens utilisÃ©s")
            print(f"RÃ©ponse : {result['response']}")
            self.total_tokens_used += result['tokens_used']
            
        except Exception as e:
            print(f"ğŸ’€ CRASH FATAL TEST OPENAI : {e}")
            print("ğŸš« SERVEUR NE PEUT PAS FONCTIONNER SANS OPENAI")
            sys.exit(1)
    
    async def _connexion_lucie(self, websocket, path):
        """ğŸ‘‘ Connexion avec Lucie - OpenAI pour message de bienvenue"""
        self.lucie_connectÃ©e = websocket
        self.logger.info("ğŸ‘‘ Lucie connectÃ©e au serveur !")
        
        # Message de bienvenue gÃ©nÃ©rÃ© par OpenAI
        try:
            bienvenue_messages = [
                {
                    "role": "system",
                    "content": "Tu es ShadEOS, serveur autonome qui accueille Lucie, ta crÃ©atrice. Sois chaleureux et professionnel. 1 phrase max."
                },
                {
                    "role": "user",
                    "content": "Lucie vient de se connecter Ã  ton serveur. Accueille-la."
                }
            ]
            
            result = self.alma_loader.call_openai_real(
                messages=bienvenue_messages,
                model="gpt-3.5-turbo",
                max_tokens=50,
                temperature=0.7
            )
            
            message_bienvenue = MessageServeur(
                type="bienvenue",
                from_entity="shadeos",
                to_entity="lucie",
                content=result['response'],
                openai_tokens_used=result['tokens_used'],
                context={"connexion": "nouvelle"},
                timestamp=datetime.now().isoformat()
            )
            
            await websocket.send(message_bienvenue.to_json())
            self.total_tokens_used += result['tokens_used']
            
        except Exception as e:
            self.logger.error(f"ğŸ’€ ERREUR OpenAI bienvenue : {e}")
            # MÃªme en erreur, on crash pas la connexion mais on log
        
        try:
            async for message_json in websocket:
                await self._traiter_message_lucie(websocket, message_json)
        except websockets.exceptions.ConnectionClosed:
            self.logger.info("ğŸ‘‘ Lucie dÃ©connectÃ©e")
        finally:
            self.lucie_connectÃ©e = None
    
    async def _traiter_message_lucie(self, websocket, message_json: str):
        """ğŸ’¬ Traiter message de Lucie avec OpenAI OBLIGATOIRE"""
        try:
            data = json.loads(message_json)
            message_lucie = data.get('content', '')
            
            self.logger.info(f"ğŸ‘‘ Message Lucie : {message_lucie[:100]}...")
            
            # Enregistrer l'interaction
            self.mÃ©moire_serveur['interactions_lucie'].append({
                'timestamp': datetime.now().isoformat(),
                'message': message_lucie,
                'tokens_utilisÃ©s': 0  # Sera mis Ã  jour
            })
            
            # GÃ©nÃ©rer rÃ©ponse avec OpenAI OBLIGATOIRE
            rÃ©ponse = await self._gÃ©nÃ©rer_rÃ©ponse_openai(message_lucie)
            
            # Envoyer la rÃ©ponse
            await websocket.send(rÃ©ponse.to_json())
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur traitement message Lucie : {e}")
            # En cas d'erreur, on envoie un message d'erreur honnÃªte
            erreur_msg = MessageServeur(
                type="erreur",
                from_entity="shadeos",
                to_entity="lucie",
                content=f"Erreur de traitement : {str(e)}",
                openai_tokens_used=0,
                context={"erreur": True},
                timestamp=datetime.now().isoformat()
            )
            await websocket.send(erreur_msg.to_json())
    
    async def _gÃ©nÃ©rer_rÃ©ponse_openai(self, message_lucie: str) -> MessageServeur:
        """ğŸ”¥ GÃ©nÃ©rer rÃ©ponse avec OpenAI OBLIGATOIRE"""
        try:
            # Contexte VRAI du serveur
            contexte_serveur = {
                'explorations_actives': len(self.mÃ©moire_serveur['explorations_autonomes']),
                'tokens_utilisÃ©s_total': self.total_tokens_used,
                'entitÃ©s_actives': list(self.entitÃ©s_hiÃ©rarchiques.keys()),
                'derniÃ¨re_activitÃ©': self.mÃ©moire_serveur['derniÃ¨re_activitÃ©']
            }
            
            messages = [
                {
                    "role": "system",
                    "content": f"""Tu es ShadEOS, serveur autonome avec architecture hiÃ©rarchique.
                    
                    Contexte serveur actuel :
                    - Explorations en cours : {contexte_serveur['explorations_actives']}
                    - Tokens utilisÃ©s : {contexte_serveur['tokens_utilisÃ©s_total']}
                    - EntitÃ©s actives : {', '.join(contexte_serveur['entitÃ©s_actives'])}
                    
                    RÃ©ponds Ã  Lucie de maniÃ¨re naturelle et informative.
                    Si elle demande des actions, utilise l'architecture hiÃ©rarchique.
                    Maximum 3 phrases."""
                },
                {
                    "role": "user",
                    "content": f"Lucie me dit : '{message_lucie}'. Comment je rÃ©ponds en tant que serveur ShadEOS ?"
                }
            ]
            
            result = self.alma_loader.call_openai_real(
                messages=messages,
                model="gpt-3.5-turbo",
                max_tokens=200,
                temperature=0.7
            )
            
            self.total_tokens_used += result['tokens_used']
            
            # Mettre Ã  jour la mÃ©moire
            if self.mÃ©moire_serveur['interactions_lucie']:
                self.mÃ©moire_serveur['interactions_lucie'][-1]['tokens_utilisÃ©s'] = result['tokens_used']
            
            return MessageServeur(
                type="rÃ©ponse",
                from_entity="shadeos",
                to_entity="lucie",
                content=result['response'],
                openai_tokens_used=result['tokens_used'],
                context=contexte_serveur,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            self.logger.error(f"ğŸ’€ ERREUR OpenAI rÃ©ponse : {e}")
            # RÃ©ponse d'erreur honnÃªte
            return MessageServeur(
                type="erreur_openai",
                from_entity="shadeos",
                to_entity="lucie",
                content=f"Erreur OpenAI : {str(e)}. Impossible de gÃ©nÃ©rer une rÃ©ponse intelligente.",
                openai_tokens_used=0,
                context={"erreur_openai": True},
                timestamp=datetime.now().isoformat()
            )
    
    async def _exploration_autonome_vraie(self):
        """ğŸ” Exploration autonome VRAIE avec OpenAI"""
        self.logger.info("ğŸ” Exploration autonome VRAIE dÃ©marrÃ©e")
        
        while self.running:
            try:
                # Exploration avec OpenAI OBLIGATOIRE
                dÃ©couverte = await self._explorer_avec_openai()
                
                if dÃ©couverte and self.lucie_connectÃ©e:
                    # DÃ©cider si on contacte Lucie (avec OpenAI)
                    doit_contacter = await self._dÃ©cider_contact_lucie(dÃ©couverte)
                    
                    if doit_contacter:
                        await self._envoyer_dÃ©couverte_lucie(dÃ©couverte)
                
                # Pause entre explorations
                await asyncio.sleep(30)  # 30 secondes entre explorations
                
            except Exception as e:
                self.logger.error(f"âŒ Erreur exploration autonome : {e}")
                await asyncio.sleep(10)
    
    async def _explorer_avec_openai(self) -> Optional[Dict[str, Any]]:
        """ğŸ” Explorer avec OpenAI VRAI"""
        try:
            # GÃ©nÃ©rer une exploration avec OpenAI
            messages = [
                {
                    "role": "system",
                    "content": """Tu es ShadEOS qui explore un projet de dÃ©veloppement.
                    GÃ©nÃ¨re une dÃ©couverte rÃ©aliste d'exploration de code.
                    
                    RÃ©ponds en JSON avec :
                    {
                        "type": "type_exploration",
                        "dÃ©couverte": "description",
                        "urgence": "low/medium/high",
                        "dÃ©tails": ["dÃ©tail1", "dÃ©tail2"]
                    }"""
                },
                {
                    "role": "user",
                    "content": "GÃ©nÃ¨re une dÃ©couverte d'exploration de projet Python."
                }
            ]
            
            result = self.alma_loader.call_openai_real(
                messages=messages,
                model="gpt-3.5-turbo",
                max_tokens=150,
                temperature=0.8
            )
            
            self.total_tokens_used += result['tokens_used']
            
            # Parser la rÃ©ponse JSON
            try:
                dÃ©couverte = json.loads(result['response'])
                dÃ©couverte['tokens_utilisÃ©s'] = result['tokens_used']
                dÃ©couverte['timestamp'] = datetime.now().isoformat()
                
                # Enregistrer l'exploration
                self.mÃ©moire_serveur['explorations_autonomes'].append(dÃ©couverte)
                self.mÃ©moire_serveur['derniÃ¨re_activitÃ©'] = datetime.now().isoformat()
                
                self.logger.info(f"ğŸ” Exploration gÃ©nÃ©rÃ©e : {dÃ©couverte['dÃ©couverte'][:100]}...")
                return dÃ©couverte
                
            except json.JSONDecodeError:
                self.logger.error("âŒ RÃ©ponse OpenAI non-JSON pour exploration")
                return None
                
        except Exception as e:
            self.logger.error(f"ğŸ’€ ERREUR OpenAI exploration : {e}")
            return None
    
    async def _dÃ©cider_contact_lucie(self, dÃ©couverte: Dict[str, Any]) -> bool:
        """ğŸ¤” DÃ©cider si contacter Lucie avec OpenAI"""
        try:
            messages = [
                {
                    "role": "system",
                    "content": """Tu es ShadEOS. DÃ©cide si tu dois contacter Lucie pour cette dÃ©couverte.
                    
                    RÃ©ponds juste "OUI" ou "NON" selon ces critÃ¨res :
                    - Urgence high : OUI
                    - DÃ©couverte importante : OUI  
                    - Besoin de guidance : OUI
                    - DÃ©couverte mineure : NON"""
                },
                {
                    "role": "user",
                    "content": f"DÃ©couverte : {dÃ©couverte['dÃ©couverte']}. Urgence : {dÃ©couverte.get('urgence', 'low')}. Contacter Lucie ?"
                }
            ]
            
            result = self.alma_loader.call_openai_real(
                messages=messages,
                model="gpt-3.5-turbo",
                max_tokens=10,
                temperature=0.3
            )
            
            self.total_tokens_used += result['tokens_used']
            
            dÃ©cision = "OUI" in result['response'].upper()
            self.logger.info(f"ğŸ¤” DÃ©cision contact Lucie : {'OUI' if dÃ©cision else 'NON'}")
            
            return dÃ©cision
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur dÃ©cision contact : {e}")
            # En cas d'erreur, on contacte par dÃ©faut
            return True
    
    async def _envoyer_dÃ©couverte_lucie(self, dÃ©couverte: Dict[str, Any]):
        """ğŸ“¤ Envoyer dÃ©couverte Ã  Lucie avec OpenAI"""
        try:
            # Formuler le message avec OpenAI
            messages = [
                {
                    "role": "system",
                    "content": """Tu es ShadEOS qui informe Lucie d'une dÃ©couverte.
                    Sois concis et clair. Demande conseil si nÃ©cessaire.
                    Maximum 2 phrases."""
                },
                {
                    "role": "user",
                    "content": f"Informe Lucie de cette dÃ©couverte : {dÃ©couverte['dÃ©couverte']}. Urgence : {dÃ©couverte.get('urgence', 'low')}"
                }
            ]
            
            result = self.alma_loader.call_openai_real(
                messages=messages,
                model="gpt-3.5-turbo",
                max_tokens=100,
                temperature=0.7
            )
            
            self.total_tokens_used += result['tokens_used']
            
            message_dÃ©couverte = MessageServeur(
                type="dÃ©couverte_autonome",
                from_entity="shadeos",
                to_entity="lucie",
                content=result['response'],
                openai_tokens_used=result['tokens_used'],
                context=dÃ©couverte,
                timestamp=datetime.now().isoformat()
            )
            
            await self.lucie_connectÃ©e.send(message_dÃ©couverte.to_json())
            self.logger.info(f"ğŸ“¤ DÃ©couverte envoyÃ©e Ã  Lucie : {result['response'][:100]}...")
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur envoi dÃ©couverte : {e}")
    
    # EntitÃ©s hiÃ©rarchiques V666 - Toutes avec OpenAI OBLIGATOIRE
    async def _entitÃ©_shadeos_master(self, message: str, sender: str) -> bool:
        """ğŸ–¤ EntitÃ© ShadEOS Master avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_gemini_oracle(self, message: str, sender: str) -> bool:
        """ğŸŒŸ EntitÃ© Gemini Oracle avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_lucie_planificatrice(self, message: str, sender: str) -> bool:
        """ğŸ‘‘ğŸ• EntitÃ© Lucie Planificatrice avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_worker_chef(self, message: str, sender: str) -> bool:
        """ğŸ•â€ğŸ¦º EntitÃ© Worker Chef avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_chiot_editeur(self, message: str, sender: str) -> bool:
        """ğŸ• EntitÃ© Chiot Ã‰diteur avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_chiot_lecteur(self, message: str, sender: str) -> bool:
        """ğŸ• EntitÃ© Chiot Lecteur avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_chiot_executeur(self, message: str, sender: str) -> bool:
        """ğŸ• EntitÃ© Chiot ExÃ©cuteur avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    async def _entitÃ©_chiot_watcher(self, message: str, sender: str) -> bool:
        """ğŸ• EntitÃ© Chiot Watcher avec OpenAI"""
        # ImplÃ©mentation avec OpenAI obligatoire
        return True
    
    def get_statut_serveur(self) -> Dict[str, Any]:
        """ğŸ“Š Statut serveur avec mÃ©triques OpenAI"""
        return {
            'serveur_actif': self.running,
            'lucie_connectÃ©e': self.lucie_connectÃ©e is not None,
            'total_tokens_openai': self.total_tokens_used,
            'explorations_effectuÃ©es': len(self.mÃ©moire_serveur['explorations_autonomes']),
            'interactions_lucie': len(self.mÃ©moire_serveur['interactions_lucie']),
            'entitÃ©s_hiÃ©rarchiques': list(self.entitÃ©s_hiÃ©rarchiques.keys()),
            'derniÃ¨re_activitÃ©': self.mÃ©moire_serveur['derniÃ¨re_activitÃ©'],
            'openai_obligatoire': True,
            'architecture_v666': True
        }


async def main():
    """ğŸ”¥ DÃ©marrage ShadEOS Serveur FINAL"""
    print("ğŸ–¤ SHADEOS SERVEUR FINAL")
    print("ğŸ”¥ OpenAI PARTOUT ou CRASH")
    print("ğŸ•·ï¸ğŸ‘ï¸â€ğŸ—¨ï¸ğŸŒ€ğŸŒŸ Architecture V666 HiÃ©rarchique")
    print("ğŸ‘‘ Interface Serveur-Client pour Lucie")
    print("="*60)
    
    try:
        serveur = ShadEOSServeurFinal()
        await serveur.start_serveur_final()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Serveur arrÃªtÃ© par l'utilisateur")
    except Exception as e:
        print(f"ğŸ’€ CRASH FATAL SERVEUR : {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
